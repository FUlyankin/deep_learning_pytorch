# Алгоритм обратного распространения ошибки

- [Скачать себе презентацию](https://github.com/FUlyankin/deep_learning_tf/raw/main/week04_backprop/nn_slides_4.pdf)


## Задание 

- На следующей паре будет квиз про градиентный спуск и матричные производные. Не забудьте подготовиться! 
- Порешайте задачки из [листочка про алгоритм обратного распространения ошибки.](https://fulyankin.github.io/deep_learning_masha_book/problem_set_03_backprop/intro_03.html) Через пару будет квиз.


## Что можно почитать и посмотреть

- [Бэкпроп по шагам](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)
- [Видос с бэкпропом за 10 минут (eng)](https://www.youtube.com/watch?v=Ilg3gGewQ5U)
- [Андрей Карпатый читает лекцию о backprop (eng),](https://www.youtube.com/watch?v=59Hbtz7XgjM) и пишет о нём [в посте (eng)](http://cs231n.github.io/optimization-2/)


## Откуда я воровал материалы

- Идею схемы с объяснением бэкпропа я [взял у Андрея Зимовнова,](https://github.com/ZEMUSHKA/mml-minor) а затем дополнил. В Tikz я её перерерисовывать упоролся.
